---
title: "Quantos Project"
output:
  html_document:
    df_print: paged
---

Code to read and analyse Quantos Fe weight files.

TODO: 
* Split off code that reads files and makes a dataset, so that we don't have to read all the files every time.
* Error checking to find that pesky file with 11 columns and the two points without times
* Data analysis- what problems do we want to solve?

```{r}
# Load libraries
library(tidyverse)
library(readxl)

# Quantos data directory path
# Starting with 2018 directory, but we'll expand it to get files
# from all the yearly directories

dir_path <- "/mnt/shared/Quantos/quantos/Dosing 2018"
```

# Load quantos data

```{r}
# Initial working excel read
data <- read_xls("/mnt/shared/Quantos/quantos/Dosing 2018/01052018.xls")

#Add some abstraction so we can load a file that's in a variable
file <- "/mnt/shared/Quantos/quantos/Dosing 2018/01052018.xls"
data <- read_xls(file)
str(data)

```

Next thing is to modify the read_xls() to tell it there's no header line
Googling with "read_xls r header" showed me that the option you need to
Set is `col_names`.

```{r}
data <- read_xls(file, col_names = c("ts", "pos", "mass", "QC"))
```

After that, you can make a list of files to read and read them into a single
dataframe by running a `for` loop (easier) or using something like `apply` or `map_dfr` (more advanced).

First step, make a vector of all the files we want to combine...
```{r}
# find all file names ending in .xls
files <- dir(path = dir_path, full.names = TRUE, pattern = "*.xls")
str(files)
```

How do we get all the data from all the years? Since we are giving `map` a list of full file paths, we just need to figure out how to get all those files from all those directories into `files`.

From the help for `dir`, there's an option called `recursive` that looks promising...

```{r}
top_dir_path <- "/mnt/shared/Quantos/quantos"
files <- dir(path = top_dir_path, full.names = TRUE, recursive = TRUE, pattern = "*.xls")
str(files)
```

OK, lets try it!

```{}
data <- files %>%
    map(read_xls) %>%    # read in all the files individually, using
                         # the function read_xls() from the readexcel package
    reduce(rbind)        # reduce with rbind into one dataframe
```

This code didn't work because we're not using col_names.
The error:
```{}
Error in match.names(clabs, names(xi)) : names do not match previous names
```

tells us that reduce couldn't combine the files
because the column names are different for every file
They're the first line of data from each file.


Let's try it again with the column names.

```{}
data <- files %>%
  map(function(file) read_xls(file, col_names = c("ts", "pos", "mass", "QC"))) %>%    # read in all the files individually, using
                       # the function read_xls() from the readexcel package
  reduce(rbind)        # reduce with rbind into one dataframe
summary(data)
```

The `map` function is a little weird here. In order to feed options to `read_xls`, we're creating an "anonymous" function within `map()`.

Also, this code fails with this error:
```{}
Error: Sheet 1 has 11 columns (11 unskipped), but `col_names` has length 4.
```

At least one of the files is corrupt and has 11 columns!

Here's our file reading and conglomerating code, put into a function to track down which excel file has 11 columns!

```{r}
readQuantosFiles <- function(files) {
  files %>%
    map(function(file) read_xls(file, col_names = c("ts", "pos", "mass", "QC"), 
                                )
        ) %>%
    reduce(rbind)        # reduce with rbind into one dataframe
}
```


Let's see what year has the problem file. We should probably just write a function to find the file, but I'm lazy so I'm going to do more work and less thinking. Actually, I'm giving up on this for now. It's a good excercise (hint, hint).

Turns out readxl has a range argument. Assuming the first four columns are the ones we want and that someone just did some fiddling with one of the files, this should work...

```{r}
readQuantosFiles <- function(files) {
  files %>%
    map(function(file) read_xls(file, col_names = c("ts", "pos", "mass", "QC"),
                                range = cell_cols("A:D") #only take first 4 columns
                                )
        ) %>%
    reduce(rbind)        # reduce with rbind into one dataframe
}
```

OK, this appears to read all the years of data correctly

```{r}
data <- readQuantosFiles(files)
str(data)
summary(data)
```

But why the heck is pos a character field and not numeric?

```{r}
unique(data$pos)
```

Well, that's interesting. Let's try and get the count of each value.

```{r}
data %>%
  group_by(pos) %>%
  summarize(N = n())
```

## Exploratory Data Analysis

We've got the data, let's look at it!

```{r}
#plot all masses over time
ggplot(data, aes(ts, mass)) + geom_point()

# plot a histogram
ggplot(data, aes(mass)) +
  geom_histogram() +
  xlim(1.5, 6)
```
```{r}
# Let's look at just 2.5mg
d <- filter(data, mass > 2, mass < 3.2)

# histogram
ggplot(d, aes(mass)) +
  geom_histogram() 

# plot over time
ggplot(d, aes(ts, mass)) + 
  geom_point(size = .5, alpha = 0.1) + 
  geom_smooth(method = 'gam', formula = y ~ s(x, bs = "cs"))

# What's 2018 look like?
d %>% 
  filter(ts > "2018-01-01") %>%
  ggplot(aes(ts, mass)) + 
    geom_point(size = .5, alpha = 0.1) + 
    geom_smooth(method = 'gam', formula = y ~ s(x, bs = "cs"))
  
```

Any relation between pos and size?

```{r}
ggplot(d, aes(pos, mass)) +
  geom_point()

ggplot(d, aes(pos, ts)) +
  geom_point()

```

